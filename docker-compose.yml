services:
  # =========================================
  # 1. HẠ TẦNG CƠ SỞ & DATA LAKE
  # =========================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.1
    container_name: zookeeper
    networks:
      - bigdata-net
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  postgres:
    image: postgres:14-alpine
    container_name: postgres
    networks:
      - bigdata-net
    ports:
      - "5432:5432"
    volumes:
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: bigdata_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U admin -d bigdata_db" ]
      interval: 5s
      timeout: 5s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:7.0.1
    container_name: kafka
    networks:
      - bigdata-net
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0

  # --- MinIO (S3 Compatible Storage) ---
  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password
    command: server /data --console-address ":9001"
    networks:
      - bigdata-net
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # Tự động tạo bucket và upload file
  minio-setup:
    image: minio/mc
    container_name: minio-setup
    networks:
      - bigdata-net
    depends_on:
      minio:
        condition: service_healthy
    volumes:
      - ./data:/data
    entrypoint: >
      /bin/sh -c "
      until (mc alias set myminio http://minio:9000 admin password); do echo '...waiting MinIO...' && sleep 1; done;
      mc mb myminio/datalake; 
      mc cp /data/airline_sentiment.csv myminio/datalake/airline_sentiment.csv;
      echo '>>> Upload to MinIO done!';
      exit 0;
      "

  simulator:
    container_name: simulator
    build:
      context: ./simulator
    networks:
      - bigdata-net
    depends_on:
      kafka:
        condition: service_started
      postgres:
        condition: service_healthy
    volumes:
      - ./data:/app/data
    restart: on-failure

  reddit-crawler:
    container_name: reddit-crawler
    build:
      context: ./reddit_crawler
    networks:
      - bigdata-net
    depends_on:
      kafka:
        condition: service_started
    restart: on-failure

  # =========================================
  # 2. SPARK CLUSTER (2 WORKERS - Distributed Batch)
  # =========================================
  spark-master:
    container_name: spark-master
    build:
      context: ./spark
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:8080"
      - "7077:7077"
    volumes:
      - ./data:/app/data
      - ./spark/jobs:/app/jobs
    networks:
      - bigdata-net
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 10s
      timeout: 5s
      retries: 5

  # Worker 1
  spark-worker-1:
    container_name: spark-worker-1
    build:
      context: ./spark
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    volumes:
      - ./data:/app/data
      - ./spark/jobs:/app/jobs
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - bigdata-net
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1024m # Cấp 1GB để thoải mái
      - SPARK_WORKER_WEBUI_PORT=8081
    ports:
      - "9091:8081" # Web UI Worker 1

  # Worker 2
  spark-worker-2:
    container_name: spark-worker-2
    build:
      context: ./spark
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    volumes:
      - ./data:/app/data
      - ./spark/jobs:/app/jobs
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - bigdata-net
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1024m
      - SPARK_WORKER_WEBUI_PORT=8081
    ports:
      - "9093:8081" # CỔNG KHÁC (9093) ĐỂ KHÔNG TRÙNG

  # Job Submitter (Chờ MinIO và 2 Worker)
  spark-submit-job:
    container_name: spark-submit-job
    build:
      context: ./spark
    command:
      - /opt/spark/bin/spark-submit
      - --master
      - spark://spark-master:7077
      - --deploy-mode
      - client
      - --conf
      - spark.executor.memory=600m
      - --conf
      - spark.driver.memory=512m
      - --conf
      - spark.cores.max=2
      - --jars
      - /opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar,/opt/spark/jars/postgresql-42.7.3.jar
      - /app/jobs/spark_batch_job.py

    networks:
      - bigdata-net
    volumes:
      - ./data:/app/data
      - ./spark/jobs:/app/jobs
    depends_on:
      spark-master:
        condition: service_healthy
      spark-worker-1:
        condition: service_started
      spark-worker-2:
        condition: service_started
      postgres:
        condition: service_healthy
      minio-setup:
        condition: service_completed_successfully # QUAN TRỌNG: Chờ upload xong
    restart: 'no'

  # =========================================
  # 3. FLINK CLUSTER (1 TaskManager - Realtime)
  # =========================================
  flink-jobmanager:
    container_name: flink-jobmanager
    build:
      context: ./flink
    ports:
      - "8088:8081"
    volumes:
      - ./flink/jobs:/opt/flink/jobs
      # - ./model:/opt/flink/model  <-- XÓA DÒNG NÀY
    networks:
      - bigdata-net
    command: >
      bash -c "
        /docker-entrypoint.sh jobmanager &
        echo 'Wait JM...'; sleep 20;
        echo 'Submit job...'; flink run -py /opt/flink/jobs/flink_stream_job.py;
        tail -f /dev/null
      "
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      # Giảm RAM xuống được rồi vì không chạy model nặng
      - FLINK_PROPERTIES=jobmanager.memory.process.size:1024m
    depends_on:
      - kafka

  # FLINK TASKMANAGER (Tương tự)
  flink-taskmanager-1:
    container_name: flink-taskmanager-1
    build:
      context: ./flink
    networks:
      - bigdata-net
    # volumes: 
    #   - ./model:/opt/flink/model <-- XÓA DÒNG NÀY
    depends_on:
      - flink-jobmanager
    command: taskmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - TASK_MANAGER_NUMBER_OF_TASK_SLOTS=2
      - FLINK_PROPERTIES=taskmanager.memory.process.size:1024m

      
  # =========================================
  # 4. WEB APP (Dashboard)
  # =========================================
  web-app:
    container_name: web-app
    build:
      context: ./web_app
    ports:
      - "5001:5000"
    networks:
      - bigdata-net
    depends_on:
      postgres:
        condition: service_healthy
      spark-submit-job:
        condition: service_completed_successfully
      flink-jobmanager:
        condition: service_started
    restart: on-failure

networks:
  bigdata-net:
    driver: bridge

volumes:
  postgres_data:
  minio_data: